# 裁判文书网爬取项目架构

## 项目概述
本项目旨在爬取裁判文书网的文书数据，并制作一个文书目录系统。这是一个非交互项目，主要涉及数据爬取、清洗、存储和目录生成。

## 技术栈
- **编程语言**: Python
- **HTTP请求**: requests + headers模拟
- **页面解析**: BeautifulSoup4 / lxml
- **数据存储**: MongoDB (localhost:27017)
- **浏览器模拟**: requests-html 或 selenium (如需要)
- **数据处理**: pandas

## 开发步骤

### 第一阶段：基础爬取架构搭建
1. **模拟浏览器请求**
   - 设置完整的HTTP请求头
   - 嵌入身份令牌 (wzws_sessionid, SESSION, wzws_cid)
   - 测试页面访问权限
   - 处理反爬机制

2. **页面结构分析**
   - 分析裁判文书网的页面结构
   - 识别文书列表的HTML结构
   - 确定分页机制
   - 识别文书详情页链接

### 第二阶段：数据抓取核心逻辑
3. **文书列表爬取**
   - 实现分页遍历
   - 提取文书基本信息（标题、案号、日期、法院等）
   - 处理动态加载内容
   - 实现请求限速和错误重试

4. **文书详情爬取**
   - 访问文书详情页
   - 提取完整文书内容
   - 处理文书格式（PDF/HTML）
   - 保存文书文件

### 第三阶段：数据存储与处理
5. **数据库设计**
   - 设计MongoDB文档结构
   - 建立索引优化查询
   - 实现数据去重机制

6. **数据清洗**
   - 标准化文书格式
   - 提取关键信息字段
   - 处理异常数据

### 第四阶段：目录生成
7. **目录系统**
   - 按法院分类
   - 按时间排序
   - 按案件类型分类
   - 生成可搜索的目录文件

### 第五阶段：测试与优化
8. **性能优化**
   - 并发请求控制
   - 内存使用优化
   - 异常处理完善

9. **测试验证**
   - 数据完整性检查
   - 重复数据验证
   - 爬取效率测试

## 数据库结构设计
```
数据库名: wenshu_db
集合: documents
文档结构:
{
    "_id": ObjectId,
    "case_number": "案号",
    "title": "文书标题", 
    "court": "审理法院",
    "date": "审理日期",
    "case_type": "案件类型",
    "content": "文书内容",
    "url": "原始链接",
    "created_at": "爬取时间",
    "file_path": "本地文件路径"
}
```

## 注意事项
1. 严格遵守robots.txt规则
2. 设置合理的请求间隔，避免对服务器造成压力
3. 处理反爬机制，如验证码、IP限制等
4. 确保数据的完整性和准确性
5. 做好异常处理和日志记录

## 开发优先级
按照上述步骤顺序进行开发，每个阶段完成后进行测试验证，确保无bug后再进行下一阶段。 